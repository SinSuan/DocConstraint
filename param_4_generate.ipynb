{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to generate text: using different decoding methods for language generation with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 12:10:32.436720: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-31 12:10:32.453398: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-31 12:10:32.453420: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-31 12:10:32.453433: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-31 12:10:32.457315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 12:10:33.466004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model/gpt2/tokenizer_config.json',\n",
       " './model/gpt2/special_tokens_map.json',\n",
       " './model/gpt2/vocab.json',\n",
       " './model/gpt2/merges.txt',\n",
       " './model/gpt2/added_tokens.json',\n",
       " './model/gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
    "# model.save_pretrained(\"./model/gpt2/\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.save_pretrained(\"./model/gpt2/\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/user_data/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 12:10:57.451424: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-31 12:10:57.469466: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-31 12:10:57.469490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-31 12:10:57.469503: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-31 12:10:57.473096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 12:10:57.922189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./model/gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•´ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# set num_return_sequences > 1\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    \n",
    "    # beam-based generation modes\n",
    "    num_beams=5,            # 1 for default (no beam search)\n",
    "    no_repeat_ngram_size=2, # setting the probability of next words that could create an already seen n-gram to 0\n",
    "    num_return_sequences=5, # the number of highest scoring beams that should be returned. (<= num_beams)\n",
    "    early_stopping=True,    # generation is finished when all beam hypotheses reached the EOS token (End Of Sequence)\n",
    "    \n",
    "    # sampling\n",
    "    do_sample=True,     # sample accroding to the score of each word\n",
    "    top_p=0.92,         # 1 for default\n",
    "    top_k=0,            # 50 for default. 0 for deactivate.\n",
    "    temperature=0.6,    # 1 for default\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, output in enumerate(outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my\n"
     ]
    }
   ],
   "source": [
    "# # encode context the generation is conditioned on\n",
    "# model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "# # generate 40 new tokens\n",
    "# greedy_output = model.generate(**model_inputs, max_new_tokens=100)\n",
    "\n",
    "# print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    # no_repeat_ngram_size=2, # setting the probability of next words that could create an already seen n-gram to 0\n",
    "    early_stopping=True     # generation is finished when all beam hypotheses reached the EOS token (End Of Sequence)\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, # setting the probability of next words that could create an already seen n-gram to 0\n",
    "    early_stopping=True     # generation is finished when all beam hypotheses reached the EOS token (End Of Sequence)\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, # setting the probability of next words that could create an already seen n-gram to 0\n",
    "    # early_stopping=True     # generation is finished when all beam hypotheses reached the EOS token (End Of Sequence)\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "\n",
      "\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "\n",
      "\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea to\n",
      "\n",
      "\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time to take a\n",
      "\n",
      "\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, # setting the probability of next words that could create an already seen n-gram to 0\n",
    "    num_return_sequences=5, # the number of highest scoring beams that should be returned. (<= num_beams)\n",
    "    early_stopping=True     # generation is finished when all beam hypotheses reached the EOS token (End Of Sequence)\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0             # 50 for default. 0 for deactivate.\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but I also love the fact that my cat is not a dog. She is a good, loving dog. I do not like to be held back by other dogs but I think that I have to\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,    # 1 for default\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guiding Text Generation with Constrained Beam Search in ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 892M/892M [01:18<00:00, 11.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "# tokenizer.save_pretrained(\"./model/t5/\")\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "# model.save_pretrained(\"./model/t5/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EX1: Constrained Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/t5/\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./model/t5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_str = \"translate English to German: How old are you?\"\n",
    "# informal : \"Wie alt bist du?\"\n",
    "#   formal : \"Wie alt sind Sie?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wie alt bist du?\n"
     ]
    }
   ],
   "source": [
    "# Traditional Beam Search\n",
    "\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wie alt sind Sie?\n"
     ]
    }
   ],
   "source": [
    "# Constrained Beam Search\n",
    "\n",
    "force_words = [\"Sie\"]\n",
    "\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wie alt sind Sie?\n"
     ]
    }
   ],
   "source": [
    "from transformers import PhrasalConstraint\n",
    "\n",
    "constraints = [\n",
    "    PhrasalConstraint(\n",
    "        tokenizer(\"Sie\", add_special_tokens=False).input_ids\n",
    "    )\n",
    "]\n",
    "\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EX2: Disjunctively Constrained Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/user_data/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 07:01:22.598662: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 07:01:23.040153: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 07:01:23.040245: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 07:01:23.040294: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 07:01:23.233393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 07:01:24.897134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./model/gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./model/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_word = \"scaredis\"\n",
    "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12008, 271]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"scaredis\"], add_prefix_space=True, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12008, 318]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"scared is\"], add_prefix_space=True, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12008, 220, 318]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"scared  is\"], add_prefix_space=True, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12008, 220, 220, 318]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"scared   is\"], add_prefix_space=True, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The soldiers, who had been ordered to leave by the commander of their own country's military intelligence agency (MIA), were told that they could not return home. The men then left for a safe place in an undisclosed location and went back into screaming scared is\n",
      "****************************************************************************************************\n",
      "The child, who was born with a congenital heart defect and is now in the care of an obstetrician or gynecologist.\n",
      " (Photo: Courtesy photo) Story Highlights The baby's mother died at age 3 after being diagnosed by scared is screams\n"
     ]
    }
   ],
   "source": [
    "force_word = \"scared is\"\n",
    "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n",
    "\n",
    "force_words_ids = [\n",
    "    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "    tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "]\n",
    "\n",
    "# æœ‰å…©å€‹èµ·å§‹å­—ä¸²ï¼Œå°±æœƒç”Ÿå‡ºå…©å¥è©±ï¼Œæ‰€ä»¥æœ€ä¸‹é¢ outputs çš„ index å¯ä»¥åˆ° 1\n",
    "starting_text = [\"The soldiers\", \"The child\"]\n",
    "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    # For warning:\n",
    "    # The attention mask and the pad token id were not set.\n",
    "    # As a consequence, you may observe unexpected behavior.\n",
    "    # Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    \n",
    "    max_new_tokens = 50,\n",
    "    # For warning:\n",
    "    # Using the model-agnostic default `max_length` (=20) to control the generation length.\n",
    "    # We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "    \n",
    "    force_words_ids=force_words_ids,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "\n",
    "print(len(outputs))\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(100 * \"*\")\n",
    "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param: Constraints\n",
    "\n",
    "huggingface - Transformers - API - MAIN CLASSES - Text Generation\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.33.3/en/main_classes/text_generation\n",
    "\n",
    "force_words_ids è·Ÿ constraintsï¼Œé€™å…©å€‹åƒæ•¸å¯ä»¥åŒæ™‚ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhrasalConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-26 09:05:28.314972: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-26 09:05:28.333126: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-26 09:05:28.333148: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-26 09:05:28.333160: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-26 09:05:28.337091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-26 09:05:28.787937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/t5/\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./model/t5/\")\n",
    "\n",
    "encoder_input_str = \"translate English to German: How old are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PhrasalConstraint\n",
    "\n",
    "constraints = [\n",
    "    PhrasalConstraint(\n",
    "        tokenizer(\"Sie is\", add_special_tokens=False).input_ids\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.generation.beam_constraints.PhrasalConstraint at 0x7f1f827e73d0>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292, 19]\n"
     ]
    }
   ],
   "source": [
    "display(constraints)\n",
    "for constraint in constraints:\n",
    "    print(constraint.token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 292\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "advance = 292\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "\n",
      "advance = 3479\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wie alt bist du? Sie isst ich schon seit 15 Jahren 456789123\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    \n",
    "    # force_words_ids=tokenizer([\"456789123\"], add_special_tokens=False).input_ids,\n",
    "    \n",
    "    constraints=constraints,    # List[Constraint]\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.generation.beam_constraints.PhrasalConstraint at 0x7f1f827e73d0>,\n",
       " <transformers.generation.beam_constraints.PhrasalConstraint at 0x7f1dbc5b0b50>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292, 19]\n",
      "[3479, 3708, 3914, 14574]\n"
     ]
    }
   ],
   "source": [
    "display(constraints)\n",
    "for constraint in constraints:\n",
    "    print(constraint.token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "67\n",
      "89\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "qqq = [3479, 3708, 3914, 14574]\n",
    "\n",
    "for id in qqq:\n",
    "    print(tokenizer.decode([id], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TemplateConstraints (not implement)\n",
    "template: ç¯„æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_text = \"The woman\"\n",
    "template = [\"the\", \"\", \"School of\", \"\", \"in\"]\n",
    "\n",
    "# possible_outputs == [\n",
    "#    \"The woman attended the Ross School of Business in Michigan.\",\n",
    "#    \"The woman was the administrator for the Harvard School of Business in MA.\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_text = \"The woman\"\n",
    "template = [\"the\", \"\", \"\", \"University\", \"\", \"in\"]\n",
    "\n",
    "# possible_outputs == [\n",
    "#    \"The woman attended the Carnegie Mellon University in Pittsburgh.\",\n",
    "# ]\n",
    "# impossible_outputs == [\n",
    "#   \"The woman attended the Harvard University in MA.\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DisjunctiveConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-26 09:04:32.948500: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-26 09:04:32.966500: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-26 09:04:32.966521: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-26 09:04:32.966534: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-26 09:04:32.970115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-26 09:04:33.425462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./model/gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./model/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DisjunctiveConstraint\n",
    "\n",
    "force_word = \"scared is\"\n",
    "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n",
    "\n",
    "constraints = [\n",
    "    DisjunctiveConstraint(\n",
    "        tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids\n",
    "    ),\n",
    "    DisjunctiveConstraint(\n",
    "        tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids\n",
    "    ),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.generation.beam_constraints.DisjunctiveConstraint at 0x7f5278dda110>,\n",
       " <transformers.generation.beam_constraints.DisjunctiveConstraint at 0x7f5285276050>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12008, 318]]\n",
      "[[8196], [26557], [14788], [25421]]\n"
     ]
    }
   ],
   "source": [
    "display(constraints)\n",
    "for constraint in constraints:\n",
    "    print(constraint.token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "\n",
      "advance = [12008]\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [8196, 26557, 14788, 25421]\n",
      "\n",
      "advance = [12008]\n",
      "2\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The soldiers, who had been ordered to leave by the commander of their own country's military intelligence agency (MIA), were told that they could not return home. The men then left for a safe place in an undisclosed location and went back into screaming scared is\n",
      "****************************************************************************************************\n",
      "The child, who was born with a congenital heart defect and is now in the care of an obstetrician or gynecologist.\n",
      " (Photo: Courtesy photo) Story Highlights The baby's mother died at age 3 after being diagnosed by scared is screams\n"
     ]
    }
   ],
   "source": [
    "# æœ‰å…©å€‹èµ·å§‹å­—ä¸²ï¼Œå°±æœƒç”Ÿå‡ºå…©å¥è©±ï¼Œæ‰€ä»¥æœ€ä¸‹é¢ outputs çš„ index å¯ä»¥åˆ° 1\n",
    "starting_text = [\"The soldiers\", \"The child\"]\n",
    "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    # For warning:\n",
    "    # The attention mask and the pad token id were not set.\n",
    "    # As a consequence, you may observe unexpected behavior.\n",
    "    # Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    \n",
    "    max_new_tokens = 50,\n",
    "    # For warning:\n",
    "    # Using the model-agnostic default `max_length` (=20) to control the generation length.\n",
    "    # We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "    \n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    \n",
    "    # do_sample = True\n",
    "    # # ValueError:\n",
    "    # # `num_beams` has to be an integer strictly greater than 1, but is 1.\n",
    "    # # For `num_beams` == 1, one should make use of `greedy_search` instead.\n",
    ")\n",
    "\n",
    "print(len(outputs))\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(100 * \"*\")\n",
    "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.generation.beam_constraints.DisjunctiveConstraint at 0x7f5278dda110>,\n",
       " <transformers.generation.beam_constraints.DisjunctiveConstraint at 0x7f5285276050>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12008, 318]]\n",
      "[[8196], [26557], [14788], [25421]]\n"
     ]
    }
   ],
   "source": [
    "display(constraints)\n",
    "for constraint in constraints:\n",
    "    print(constraint.token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocConstraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .DocConstraint import DocConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TemplateConstraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TemplateConstraint' from 'transformers' (/user_data/.local/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemplateConstraint\n\u001b[1;32m      3\u001b[0m force_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscared\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m force_flexible \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreamed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TemplateConstraint' from 'transformers' (/user_data/.local/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import TemplateConstraint\n",
    "\n",
    "force_word = \"scared\"\n",
    "force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n",
    "\n",
    "constraints = [\n",
    "    TemplateConstraint(\n",
    "        tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids\n",
    "    )\n",
    "    \n",
    "]\n",
    "\n",
    "# æœ‰å…©å€‹èµ·å§‹å­—ä¸²ï¼Œå°±æœƒç”Ÿå‡ºå…©å¥è©±ï¼Œæ‰€ä»¥æœ€ä¸‹é¢ outputs çš„ index å¯ä»¥åˆ° 1\n",
    "starting_text = [\"The soldiers\", \"The child\"]\n",
    "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    # For warning:\n",
    "    # The attention mask and the pad token id were not set.\n",
    "    # As a consequence, you may observe unexpected behavior.\n",
    "    # Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    \n",
    "    max_new_tokens = 50,\n",
    "    # For warning:\n",
    "    # Using the model-agnostic default `max_length` (=20) to control the generation length.\n",
    "    # We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "\n",
    "print(len(outputs))\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(100 * \"*\")\n",
    "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([1,2,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
